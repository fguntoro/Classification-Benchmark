TrainTestSplit:
  random_state: &seed 33
  test_size: 0.2
  # shuffle: True
  # stratify: None

SplitterClass:
  module: sklearn.model_selection
  splitter: StratifiedShuffleSplit
  params:
    n_splits: 5
    random_state: 8

CrossValidation:
  # refit: 'mean_squared_log_error'
  n_jobs: 8
  return_train_score: True
  verbose: 2

Models:
  ###########################################
  # Classification (Resistant/ Susceptible) #
  ###########################################

  # Methods included in the Scikit-Learn Package
  # For further information, please refer to https://scikit-learn.org/stable/supervised_learning.html#supervised-learning

  LR:
    module: sklearn.linear_model
    model: LogisticRegression
    params:
      n_jobs: -1
      penalty: "l2"
      class_weight: "balanced"
      solver: "lbfgs"
      C: 1000000000  # Set a large value for C to effectively disable regularization
      max_iter: 10000
    cv:
      max_iter: np.logspace(3, 5, base=10, num=3, dtype=int)
      solver: ["lbfgs", "saga"]

  LR_l1:
    module: sklearn.linear_model
    model: LogisticRegression
    params:
      n_jobs: -1
      penalty: "l1"
      class_weight: "balanced"
      solver: "liblinear"
      max_iter: 10000
    cv:
      max_iter: np.logspace(3, 5, base=10, num=3, dtype=int)
      solver: ["liblinear", "saga"]
      C: np.logspace(-4, 4, base=10, num=5)

  LR_l2:
    module: sklearn.linear_model
    model: LogisticRegression
    params:
      penalty: "l2"
      n_jobs: -1
      class_weight: "balanced"
      solver: "liblinear"
      max_iter: 10000
    cv:
      max_iter: np.logspace(3, 5, base=10, num=3, dtype=int)
      solver: ["newton-cg", "sag", "lbfgs"]
      C: np.logspace(-4, 4, base=10, num=5)

  LR_elasticnet:
    module: sklearn.linear_model
    model: LogisticRegression
    params:
      penalty: "elasticnet"
      solver: "saga"
      n_jobs: -1
      class_weight: "balanced"
      l1_ratio: 0.5
    cv:
      #max_iter: np.logspace(2, 4, base=10, num=3, dtype=int)
      #l1_ratio: [0.25, 0.5, 0.75]
      C: np.logspace(-2, 2, base=10, num=5)

  SVM_l1:
    module: sklearn.svm
    model: LinearSVC
    params:
      penalty: "l1"
      dual: False
      class_weight: "balanced"
      max_iter: 10000
    cv:
      max_iter: np.logspace(3, 5, base=10, num=3, dtype=int)
      loss: ["squared_hinge"]
      C: np.logspace(-4, 4, base=10, num=5)

  SVM_l2:
    module: sklearn.svm
    model: LinearSVC
    params:
      penalty: "l2"
      class_weight: "balanced"
      max_iter: 10000
    cv:
      max_iter: np.logspace(3, 5, base=10, num=3, dtype=int)
      loss: ["squared_hinge", "hinge"]
      C: np.logspace(-4, 4, base=10, num=5)

  RFC:
    module: sklearn.ensemble
    model: RandomForestClassifier
    params:
      n_jobs: -1
      class_weight: "balanced"
    cv:
      n_estimators: [100, 300, 500]
      #max_depth: [5, 10, 15]
      #min_samples_split: [0.01, 0.05, 0.1, 0.15]
      #min_samples_leaf: [0.01, 0.05, 0.1, 0.15]

  DT:
    module: sklearn.tree
    model: DecisionTreeClassifier
    params:
      class_weight: "balanced"
      max_features: "auto"
    cv:
      max_depth: [5, 10, 15, 30]
      min_samples_split: [2, 5, 10, 15]
      min_samples_leaf: [1, 2, 5, 10]

  ET:
    module: sklearn.ensemble
    model: ExtraTreesClassifier
    params:
      n_jobs: -1
      class_weight: "balanced"
    cv:
      n_estimators: [100, 300, 500, 1000]
      max_depth: [5, 10, 15, 30]
      min_samples_split: [2, 5, 10, 15]
      min_samples_leaf: [1, 2, 5, 10]

  ADB:
    module: sklearn.ensemble
    model: AdaBoostClassifier
    params:
      n_estimators: 50
      learning_rate: 0.0001
    cv:
      n_estimators: [50, 100, 500, 1000]
      learning_rate: [0.0001, 0.001, 0.01, 0.1, 1.0]

  GBTC:
    module: sklearn.ensemble
    model: GradientBoostingClassifier
    params:
      max_features: 1.0
      n_iter_no_change: 20
    cv:
      learning_rate: [0.001, 0.01, 0.1, 1]
      n_estimators: [100, 300, 500, 1000]
      min_samples_split: [2, 5, 10, 15]
      max_depth: [5, 10, 15, 30]

  SGDC_l1:
    module: sklearn.linear_model
    model: SGDClassifier
    params:
      n_jobs: -1
      class_weight: "balanced"
      penalty: "l1"
      loss: "log_loss"
    cv:
      max_iter: np.logspace(1, 5, base=10, num=5, dtype=int)
      alpha: np.logspace(-4, 0, base=10, num=5)

  SGDC_l2:
    module: sklearn.linear_model
    model: SGDClassifier
    params:
      n_jobs: -1
      class_weight: "balanced"
      penalty: "l2"
      loss: "log_loss"
    cv:
      max_iter: np.logspace(1, 5, base=10, num=5, dtype=int)
      alpha: np.logspace(-4, 0, base=10, num=5)

  SGDC_elasticnet:
    module: sklearn.linear_model
    model: SGDClassifier
    params:
      n_jobs: -1
      class_weight: "balanced"
      penalty: "elasticnet"
      loss: "log_loss"
    cv:
      max_iter: np.logspace(2, 5, base=10, num=4, dtype=int)
      alpha: np.logspace(-4, 0, base=10, num=5)
      l1_ratio: [0.25, 0.5, 0.75]

  KNN:
    module: sklearn.neighbors
    model: KNeighborsClassifier
    params:
      n_jobs: -1
    cv:
      n_neighbors: [3, 5, 10, 15]
      leaf_size: [20, 30, 40]
      p: [1, 2]
      weights: ["uniform", "distance"]

  GNB:
    module: sklearn.naive_bayes
    model: GaussianNB
    params:
      var_smoothing: 0.000001
    cv:
      var_smoothing: np.logspace(-9, 0, base=10, num=10)

  CNB:
    module: sklearn.naive_bayes
    model: ComplementNB
    params:
      alpha: 0.01
    cv:
      alpha: [0.01, 0.1, 0.5, 1.0, 10.0]
      norm: [False, True]

  #################################################
  # Regression (Minimum Inhibitory Concentration) #
  #################################################

  # Methods included in the Scikit-Learn Package
  # For further information, please refer to https://scikit-learn.org/stable/supervised_learning.html#supervised-learning

  LinR:
    module: sklearn.linear_model
    model: LinearRegression
    params:
      fit_intercept: True

  LinR_l1:
    module: sklearn.linear_model
    model: Lasso
    params:
      alpha: 1.0
    cv:
      alpha: [0.5, 1, 2]

  LinR_l2:
    module: sklearn.linear_model
    model: Ridge
    params:
      alpha: 1.0
    cv:
      alpha: [0.5, 1, 2]

  LinR_elasticnet:
    module: sklearn.linear_model
    model: ElasticNet
    params:
      alpha: 1.0
    cv:
      alpha: [0.5, 1, 2]
      l1_ratio: [0.25, 0.5, 0.75]

  SVMR:
    module: sklearn.svm
    model: SVR
    params:
      kernel: "rbf"
    cv:
      max_iter: np.logspace(1, 5, base=10, num=5, dtype=int)
      C: np.logspace(-4, 4, base=10, num=5)

  GBTR:
    module: sklearn.ensemble
    model: GradientBoostingRegressor
    params:
      max_features: "auto"
      n_iter_no_change: 20
    cv:
      learning_rate: [0.001, 0.01, 0.1, 1]
      n_estimators: [100, 300, 500, 1000]
      min_samples_split: [2, 5, 10, 15]
      max_depth: [5, 10, 15, 30]

  RFR:
    module: sklearn.ensemble
    model: RandomForestRegressor
    params:
      n_jobs: -1
    cv:
      n_estimators: [100, 300, 500, 1000]
      max_depth: [5, 10, 15, 30]
      min_samples_split: [2, 5, 10, 15]
      min_samples_leaf: [1, 2, 5, 10]
