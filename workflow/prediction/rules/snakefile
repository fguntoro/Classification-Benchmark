import yaml
import glob
import os.path
from itertools import product
import pandas as pd

configfile: "workflow/prediction/rules/models_config.yml"


with open("config.yaml") as file:
    global_config = yaml.load(file, yaml.FullLoader)

input_dir = global_config["INPUT_DIR"] + "/"
file_label = global_config["FILE_LABEL"]
output_dir = global_config["OUTPUT_DIR"] + "/"


def get_groups(config):
    if config["GROUP"] == "all":
        df = pd.read_csv(config["FILE_LABEL"], index_col=0)
        groups = df.columns.tolist()
    else:
        groups = config["GROUP"]
    return groups


rule build_model:
   input:
       conf = "workflow/prediction/rules/models_config.yml",
        label_train = output_dir + \
            "{data}/{group}/preprocessing/split_data/y_train.csv",
   output:
       output_dir + "{data}/{group}/prediction/models/{method}/{model}.joblib",
   conda:
       "../../prediction/envs/env.yml"
   script:
       "../../prediction/scripts/build_model.py"

def get_mode(y_train):
    if all(isinstance(item, str) for item in y_train):
        unique_values = set(y_train)
        if len(unique_values) == 2:
            return "Classification", True
        else:
            return "Classification", False

    elif all(isinstance(item, (int, float)) for item in y_train):
        if all(item == 0 or item == 1 for item in y_train):
            return "Classification", True
        else:
            return "Regression", False

# def get_models(global_config, label_file):
#     y_train = pd.read_csv(label_file)[1]
#     mode = get_mode(y_train)
#     methods = global_config["METHODS"]
#     print(mode)


rule evaluation:
    input:
        data_train = output_dir + \
            "{data}/{group}/preprocessing/split_data/X_train.csv",
        label_train = output_dir + \
            "{data}/{group}/preprocessing/split_data/y_train.csv",
        data_test = output_dir + \
            "{data}/{group}/preprocessing/split_data/X_test.csv",
        label_test = output_dir + \
            "{data}/{group}/preprocessing/split_data/y_test.csv",
        model =        output_dir + "{data}/{group}/prediction/models/{method}/{model}.joblib",

        conf = "workflow/prediction/rules/models_config.yml",
    output:
        evaluation = output_dir + \
            "{data}/{group}/prediction/results/{method}/{model}-nofeatureselection.csv",
        fitted_model = output_dir + \
            "{data}/{group}/prediction/results/{method}/{model}-nofeatureselection.joblib",
    conda:
        "../../prediction/envs/env.yml"
    threads: 8
    script:
        "../../prediction/scripts/evaluation.py"


use rule evaluation as evaluate_feature_selection with:
    input:
        data_train = output_dir + \
            "{data}/{group}/preprocessing/split_data/X_train.csv",
        label_train = output_dir + \
            "{data}/{group}/preprocessing/split_data/y_train.csv",
        data_test = output_dir + "{data}/{group}/preprocessing/split_data/X_test.csv",
        label_test = output_dir + \
            "{data}/{group}/preprocessing/split_data/y_test.csv",
        features = output_dir + \
            "{data}/{group}/feature_selection/{feature_selection}/{feature_selection}-{estimator}.csv",
        model = output_dir + \
            output_dir + "{data}/{group}/prediction/models/{method}/{model}.joblib",
        conf = "workflow/prediction/rules/models_config.yml",
    wildcard_constraints:
        feature_selection = "sklearn_.*"
    output:
        evaluation = output_dir + \
            "{data}/{group}/prediction/results/{method}/{model}-{feature_selection}-{estimator}.csv",
        fitted_model = output_dir + \
            "{data}/{group}/prediction/results/{method}/{model}-{feature_selection}-{estimator}.joblib",

use rule evaluation as evaluate_boruta with:
    input:
        data_train = output_dir + \
            "{data}/{group}/preprocessing/split_data/X_train.csv",
        label_train = output_dir + \
            "{data}/{group}/preprocessing/split_data/y_train.csv",
        data_test = output_dir + "{data}/{group}/preprocessing/split_data/X_test.csv",
        label_test = output_dir + \
            "{data}/{group}/preprocessing/split_data/y_test.csv",
        features = output_dir + \
            "{data}/{group}/feature_selection/{feature_selection}/{feature_selection}-{estimator}.csv",
        model = output_dir + "{data}/{group}/prediction/models/{method}.joblib",
        conf = "workflow/prediction/rules/models_config.yml",
    wildcard_constraints:
        feature_selection = "boruta"
    output:
        evaluation = output_dir + \
            "{data}/{group}/prediction/results/{method}/{feature_selection}-{estimator}.csv",
        fitted_model = output_dir + \
            "{data}/{group}/prediction/results/{method}/{feature_selection}-{estimator}.joblib",


use rule evaluation as evaluate_stability with:
    input:
        data_train = output_dir + \
            "{data}/{group}/preprocessing/split_data/X_train.csv",
        label_train = output_dir + \
            "{data}/{group}/preprocessing/split_data/y_train.csv",
        data_test = output_dir + "{data}/{group}/preprocessing/split_data/X_test.csv",
        label_test = output_dir + \
            "{data}/{group}/preprocessing/split_data/y_test.csv",
        features = output_dir + \
            "{data}/{group}/feature_selection/stability/stability.csv",
        model = output_dir + output_dir + "{data}/{group}/prediction/models/{method}/{model}.joblib",
        conf = "workflow/prediction/rules/models_config.yml",
    output:
        evaluation = output_dir + \
            "{data}/{group}/prediction/results/{method}/{model}-stability.csv",
        fitted_model = output_dir + \
            "{data}/{group}/prediction/results/{method}/{model}-stability.joblib",

#  # the checkpoint that shall trigger re-evaluation of the DAG
# checkpoint check_results:
#     input: output_dir + \
#         "{data}/{group}/prediction/results/{method}/{analysis}.csv"
#     output: 
#         directory(
#             output_dir + "{data}/{group}/prediction/check_results/{method}/{analysis}")
#     shell:
#         "cp {input} {output}"

# # an intermediate rule
# rule intermediate:
#     input:
#         output_dir + \
#         "{data}/{group}/prediction/results/{method}/{analysis}.csv"
#     output:
#         output_dir + \
#         "{data}/{group}/prediction/check_results/{method}/{analysis}.csv"
#     shell:
#         "cp {input} {output}"

# def aggregate_input(wildcards):
#     checkpoint_output = checkpoints.check_results.get(**wildcards).output[0]
#     print(checkpoint_output)
#     return expand(output_dir + \
#                   "{data}/{group}/prediction/check_results/{method}/{analysis}.csv",
#                   data=config["FILE_DATA"],
#                   group=get_groups(config),
#                   method=config["METHODS"],
#                   analysis=glob_wildcards(os.path.join(checkpoint_output, "{analysis}.csv")).analysis)



# rule aggregate_summary_bymethod:
#     input:
#         aggregate_input
#     output:
#         output_dir + "{data}/{group}/prediction/results/{method}/{analysis}/summary.csv",
#     script:
#         "../../prediction/scripts/aggregate.py"


def aggregate_feature_selection(datas, groups, methods, feature_selections, estimators):
    files = list()
    for data in datas:
        for group in groups:

            y_train = pd.read_csv(file_label)[group]
            print(y_train)
            mode, isBinary = get_mode(y_train)

            for method in methods:
                if method == "linear_model":
                    if mode == "Classification" and isBinary:
                        models = ["LR"]
                    elif mode == "Regression":
                        models = ["LinR"]
                        
                for model in models:
                    for feature_selection in feature_selections:
                        if feature_selection == "stability":
                            path = output_dir + \
                                "{data}/{group}/prediction/results/{method}/{model}-{feature_selection}.csv"
                            files.append(path.format(
                                data=data,
                                group=group,
                                method=method,
                                model=model,
                                feature_selection=feature_selection))
                        elif feature_selection == "boruta":
                            path = output_dir + \
                                "{data}/{group}/prediction/results/{method}/{feature_selection}-{estimator}.csv"
                            for estimator in estimators:
                                if estimator in ["DT", "RFC", "ET"]:
                                    files.append(path.format(
                                        data=data,
                                        group=group,
                                        method=method,
                                        model=model,
                                        feature_selection=feature_selection,
                                        estimator=estimator))
                        else:
                            path = output_dir + \
                                "{data}/{group}/prediction/results/{method}/{feature_selection}-{estimator}.csv"
                            for estimator in estimators:
                                files.append(path.format(
                                    data=data,
                                    group=group,
                                    method=method,
                                    model=model,
                                    feature_selection=feature_selection,
                                    estimator=estimator))
    
    print(files)

    return files




rule aggregate_summary:
    input:
        # lambda wildcards: expand(
        #     output_dir +
        #     "{data}/{group}/prediction/results/{method}/{model}-nofeatureselection.csv",
        #     method=global_config["METHODS"],
        #     data=wildcards.data,
        #     group=wildcards.group,
        #     allow_missing=True,
        # ),
        aggregate_feature_selection(
            datas=global_config["FILE_DATA"],
            groups=global_config["GROUP"],
            methods=global_config["METHODS"],
            feature_selections=global_config["FEATURE_SELECTION"],
            estimators=global_config["ESTIMATOR"],
        ),
    params:
        config = config,
    output:
        output_dir + "{data}/{group}/prediction/summary.csv",
    script:
        "../../prediction/scripts/aggregate.py"
